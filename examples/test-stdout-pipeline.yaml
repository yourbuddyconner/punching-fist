# Test example for Phase 1 MVP with stdout sink
# This demonstrates the full Source â†’ Workflow â†’ Sink pipeline
# using stdout for easy testing without external dependencies
---
# Source: Webhook endpoint for test alerts
apiVersion: punchingfist.io/v1alpha1
kind: Source
metadata:
  name: test-webhook-source
  namespace: punching-fist
spec:
  type: webhook
  config:
    path: "/webhook/test"
    filters:
      severity: ["critical", "warning"]
      alertname: ["PodCrashLooping", "HighCPUUsage", "HighMemoryUsage"]
  triggerWorkflow: "test-investigation"
  context:
    environment: "test"
    testMode: "true"
---
# Workflow: Agent investigation for test alerts
apiVersion: punchingfist.io/v1alpha1
kind: Workflow
metadata:
  name: test-investigation
  namespace: punching-fist
spec:
  runtime:
    image: "punchingfist/runtime:v1.0.0"
    llmConfig:
      provider: "anthropic"  # or "openai", "mock"
      model: "claude-3-sonnet-20240229"
    environment:
      PROMETHEUS_URL: "http://prometheus-operated:9090"
      
  steps:
    - name: "investigate-alert"
      type: "agent"
      goal: |
        Investigate this test alert: {{ .source.data.alertname }}
        
        Alert details:
        - Severity: {{ .source.data.severity }}
        - Labels: {{ .source.data.labels | toJSON }}
        - Description: {{ .source.data.annotations.description }}
        
        Please:
        1. Identify what's happening with the affected pod/service
        2. Check relevant metrics if available
        3. Determine the root cause
        4. Suggest remediation steps
        
        Focus on the test workloads in the test-workloads namespace.
      tools:
        - name: "kubectl"
          description: "Kubernetes command line tool"
        - name: "promql"
          description: "Query Prometheus metrics"
      maxIterations: 10
      timeoutMinutes: 5
      
  outputs:
    - name: "investigation_summary"
      value: "{{ .steps.investigate-alert.result.summary }}"
    - name: "findings"
      value: "{{ .steps.investigate-alert.result.findings }}"
    - name: "root_cause"
      value: "{{ .steps.investigate-alert.result.root_cause }}"
    - name: "recommendations"
      value: "{{ .steps.investigate-alert.result.recommendations }}"
  
  sinks:
    - "stdout-debug"
---
# Sink: Stdout for easy testing
apiVersion: punchingfist.io/v1alpha1
kind: Sink
metadata:
  name: stdout-debug
  namespace: punching-fist
spec:
  type: stdout
  config:
    template: |
      ========================================
      ðŸ¤– PUNCHING FIST INVESTIGATION RESULTS
      ========================================
      
      Alert: {{ .source.data.alertname }}
      Severity: {{ .source.data.severity }}
      Time: {{ .workflow.completedAt }}
      
      ----------------------------------------
      INVESTIGATION SUMMARY:
      ----------------------------------------
      {{ .workflow.outputs.investigation_summary }}
      
      ----------------------------------------
      DETAILED FINDINGS:
      ----------------------------------------
      {{ .workflow.outputs.findings }}
      
      ----------------------------------------
      ROOT CAUSE ANALYSIS:
      ----------------------------------------
      {{ .workflow.outputs.root_cause }}
      
      ----------------------------------------
      RECOMMENDATIONS:
      ----------------------------------------
      {{ .workflow.outputs.recommendations }}
      
      ========================================
      Workflow: {{ .workflow.name }}
      Duration: {{ .workflow.duration }}
      Status: {{ .workflow.status }}
      ========================================
---
# Example AlertManager webhook payload for testing
# Send this to http://localhost:8080/webhook/test
# {
#   "version": "4",
#   "groupKey": "{}:{alertname=\"PodCrashLooping\"}",
#   "truncatedAlerts": 0,
#   "status": "firing",
#   "receiver": "punchingfist",
#   "groupLabels": {
#     "alertname": "PodCrashLooping"
#   },
#   "commonLabels": {
#     "alertname": "PodCrashLooping",
#     "severity": "critical"
#   },
#   "commonAnnotations": {
#     "description": "Pod crashloop-app in namespace test-workloads has been restarting frequently"
#   },
#   "externalURL": "http://alertmanager:9093",
#   "alerts": [
#     {
#       "status": "firing",
#       "labels": {
#         "alertname": "PodCrashLooping",
#         "severity": "critical",
#         "namespace": "test-workloads",
#         "pod": "crashloop-app",
#         "container": "failing-app"
#       },
#       "annotations": {
#         "description": "Pod crashloop-app in namespace test-workloads has been restarting frequently",
#         "summary": "Pod is crash looping"
#       },
#       "startsAt": "2024-01-15T10:00:00Z",
#       "endsAt": "0001-01-01T00:00:00Z",
#       "generatorURL": "http://prometheus:9090/alerts",
#       "fingerprint": "a1b2c3d4e5f6g7h8"
#     }
#   ]
# } 